{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "858c94bd-d41b-4548-9332-013bff1ff0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting biopython\n",
      "  Downloading biopython-1.85-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy in /home/azstephe/miniconda3/envs/enformer_env/lib/python3.9/site-packages (from biopython) (1.19.5)\n",
      "Downloading biopython-1.85-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: biopython\n",
      "Successfully installed biopython-1.85\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install biopython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b930f27-0ff2-4f16-93db-e215812a9569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "def one_hot_encode(sequence: str,\n",
    "                   alphabet: str = 'ACGT',\n",
    "                   neutral_alphabet: str = 'N',\n",
    "                   neutral_value: Any = 0,\n",
    "                   dtype=np.float32) -> np.ndarray:\n",
    "  \"\"\"One-hot encode sequence.\"\"\"\n",
    "  def to_uint8(string):\n",
    "    return np.frombuffer(string.encode('ascii'), dtype=np.uint8)\n",
    "  hash_table = np.zeros((np.iinfo(np.uint8).max, len(alphabet)), dtype=dtype)\n",
    "  hash_table[to_uint8(alphabet)] = np.eye(len(alphabet), dtype=dtype)\n",
    "  hash_table[to_uint8(neutral_alphabet)] = neutral_value\n",
    "  hash_table = hash_table.astype(dtype)\n",
    "  return hash_table[to_uint8(sequence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3636306-5fd0-4624-b520-3feb15721504",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "GPU Details: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-06 13:22:27.537329: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2026-01-06 13:22:27.565620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:3b:00.0 name: NVIDIA RTX A6000 computeCapability: 8.6\n",
      "coreClock: 1.8GHz coreCount: 84 deviceMemorySize: 47.54GiB deviceMemoryBandwidth: 715.34GiB/s\n",
      "2026-01-06 13:22:27.565657: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2026-01-06 13:22:27.774236: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2026-01-06 13:22:27.774326: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2026-01-06 13:22:27.842019: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2026-01-06 13:22:27.916030: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2026-01-06 13:22:27.962079: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2026-01-06 13:22:28.000950: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2026-01-06 13:22:28.020184: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2026-01-06 13:22:28.020978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpu_devices))\n",
    "\n",
    "if gpu_devices:\n",
    "    print(\"GPU Details:\", gpu_devices)\n",
    "else:\n",
    "    print(\"TensorFlow is NOT using the GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05434f94-b1f5-4d5f-b97a-4e152cdb6e3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "species = 'mouse'\n",
    "narrowPeak_file = species + '_liver_pos_ALL.narrowPeak'\n",
    "\n",
    "chrom_dir = '/home/azstephe/liverRegression/regression_liver/data/chrom_sizes/'\n",
    "chrom_sizes = pd.read_csv(chrom_dir + 'mm10.chrom.sizes', sep='\\t', names=['chrom', 'chrom_len'])\n",
    "chrom_map = dict(zip(chrom_sizes['chrom'], chrom_sizes['chrom_len']))\n",
    "\n",
    "with open('/home/azstephe/liverRegression/regression_liver/data/chrom_sizes/' + species + '_chrom_sizes.json', 'w') as f:\n",
    "    json.dump(chrom_map, f)\n",
    "\n",
    "# narrowPeak_dir = '/home/azstephe/liverRegression/regression_liver/data/raw/'\n",
    "\n",
    "# cols = ['chrom', 'start', 'end', 'name', 'score', 'strand', 'signal', 'p', 'q', 'peak_offset']\n",
    "# df = pd.read_csv(narrowPeak_dir + narrowPeak_file, sep='\\t', names=cols)\n",
    "\n",
    "# extension = 196608\n",
    "# df['summit'] = df['start'] + df['peak_offset']\n",
    "# df['new_start'] = df['summit'] - extension\n",
    "# df['new_end'] = df['summit'] + extension\n",
    "\n",
    "# # 4. Filter for boundaries\n",
    "# # Get the max length for the specific chromosome of each peak\n",
    "# df['max_len'] = df['chrom'].map(chrom_map)\n",
    "\n",
    "# # Keep only if start >= 0 AND end <= chromosome length\n",
    "# valid_sections = df[(df['new_start'] >= 0) & (df['new_end'] <= df['max_len'])].copy()\n",
    "\n",
    "# print(f\"Original peaks: {len(df)}\")\n",
    "# print(f\"Peaks kept: {len(valid_sections)}\")\n",
    "# print(f\"Peaks lost: {len(df) - len(valid_sections)}\")\n",
    "\n",
    "# outdir = '/home/azstephe/liverRegression/regression_liver/data/enformer_inputs/'\n",
    "# valid_sections[['chrom', 'new_start', 'new_end', 'name', 'signal']].to_csv(outdir + narrowPeak_file, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c89adb01-c16a-472c-960a-df66dc3bbcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = 'macaque'\n",
    "\n",
    "chrom_dir = '/home/azstephe/liverRegression/regression_liver/data/chrom_sizes/'\n",
    "chrom_sizes = pd.read_csv(chrom_dir + 'rheMac8.chrom.sizes', sep='\\t', names=['chrom', 'chrom_len'])\n",
    "chrom_map = dict(zip(chrom_sizes['chrom'], chrom_sizes['chrom_len']))\n",
    "with open('/home/azstephe/liverRegression/regression_liver/data/chrom_sizes/' + species + '_chrom_sizes.json', 'w') as f:\n",
    "    json.dump(chrom_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "37ef2d0b-b44f-43a4-92e9-0f18e248b5a1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "species = 'pig'\n",
    "narrowPeak_file = species + '_liver_pos_ALL.narrowPeak'\n",
    "\n",
    "chrom_dir = '/home/azstephe/liverRegression/regression_liver/data/chrom_sizes/'\n",
    "chrom_sizes = pd.read_csv(chrom_dir + 'susScr3.chrom.sizes', sep='\\t', names=['chrom', 'chrom_len'])\n",
    "chrom_map = dict(zip(chrom_sizes['chrom'], chrom_sizes['chrom_len']))\n",
    "with open('/home/azstephe/liverRegression/regression_liver/data/chrom_sizes/' + species + '_chrom_sizes.json', 'w') as f:\n",
    "    json.dump(chrom_map, f)\n",
    "# narrowPeak_dir = '/home/azstephe/liverRegression/regression_liver/data/raw/'\n",
    "\n",
    "# cols = ['chrom', 'start', 'end', 'name', 'score', 'strand', 'signal', 'p', 'q', 'peak_offset']\n",
    "# df = pd.read_csv(narrowPeak_dir + narrowPeak_file, sep='\\t', names=cols)\n",
    "\n",
    "# extension = 196608\n",
    "# df['summit'] = df['start'] + df['peak_offset']\n",
    "# df['new_start'] = df['summit'] - extension\n",
    "# df['new_end'] = df['summit'] + extension\n",
    "\n",
    "# # 4. Filter for boundaries\n",
    "# # Get the max length for the specific chromosome of each peak\n",
    "# df['max_len'] = df['chrom'].map(chrom_map)\n",
    "\n",
    "# # Keep only if start >= 0 AND end <= chromosome length\n",
    "# valid_sections = df[(df['new_start'] >= 0) & (df['new_end'] <= df['max_len'])].copy()\n",
    "\n",
    "# print(f\"Original peaks: {len(df)}\")\n",
    "# print(f\"Peaks kept: {len(valid_sections)}\")\n",
    "# print(f\"Peaks lost: {len(df) - len(valid_sections)}\")\n",
    "\n",
    "# outdir = '/home/azstephe/liverRegression/regression_liver/data/enformer_inputs/'\n",
    "# valid_sections[['chrom', 'new_start', 'new_end', 'name', 'signal']].to_csv(outdir + narrowPeak_file, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21dd666f-d97e-41f1-8ac9-5deaebc1cc66",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "species = 'rat'\n",
    "narrowPeak_file = species + '_liver_pos_ALL.narrowPeak'\n",
    "\n",
    "chrom_dir = '/home/azstephe/liverRegression/regression_liver/data/chrom_sizes/'\n",
    "chrom_sizes = pd.read_csv(chrom_dir + 'rn6.chrom.sizes', sep='\\t', names=['chrom', 'chrom_len'])\n",
    "chrom_map = dict(zip(chrom_sizes['chrom'], chrom_sizes['chrom_len']))\n",
    "with open('/home/azstephe/liverRegression/regression_liver/data/chrom_sizes/' + species + '_chrom_sizes.json', 'w') as f:\n",
    "    json.dump(chrom_map, f)\n",
    "# narrowPeak_dir = '/home/azstephe/liverRegression/regression_liver/data/raw/'\n",
    "\n",
    "# cols = ['chrom', 'start', 'end', 'name', 'score', 'strand', 'signal', 'p', 'q', 'peak_offset']\n",
    "# df = pd.read_csv(narrowPeak_dir + narrowPeak_file, sep='\\t', names=cols)\n",
    "\n",
    "# extension = 196608\n",
    "# df['summit'] = df['start'] + df['peak_offset']\n",
    "# df['new_start'] = df['summit'] - extension\n",
    "# df['new_end'] = df['summit'] + extension\n",
    "\n",
    "# # 4. Filter for boundaries\n",
    "# # Get the max length for the specific chromosome of each peak\n",
    "# df['max_len'] = df['chrom'].map(chrom_map)\n",
    "\n",
    "# # Keep only if start >= 0 AND end <= chromosome length\n",
    "# valid_sections = df[(df['new_start'] >= 0) & (df['new_end'] <= df['max_len'])].copy()\n",
    "\n",
    "# print(f\"Original peaks: {len(df)}\")\n",
    "# print(f\"Peaks kept: {len(valid_sections)}\")\n",
    "# print(f\"Peaks lost: {len(df) - len(valid_sections)}\")\n",
    "\n",
    "# outdir = '/home/azstephe/liverRegression/regression_liver/data/enformer_inputs/'\n",
    "# valid_sections[['chrom', 'new_start', 'new_end', 'name', 'signal']].to_csv(outdir + narrowPeak_file, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4faf57e8-2dac-46d7-8723-29a99ac311be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "species = 'cow'\n",
    "narrowPeak_file = species + '_liver_pos_ALL_refSeqName.bed'\n",
    "\n",
    "chrom_dir = '/home/azstephe/liverRegression/regression_liver/data/chrom_sizes/'\n",
    "chrom_sizes = pd.read_csv(chrom_dir + 'Btau_5.0.1.chrom.sizes', sep='\\t', names=['chrom', 'chrom_len'])\n",
    "chrom_map = dict(zip(chrom_sizes['chrom'], chrom_sizes['chrom_len']))\n",
    "with open('/home/azstephe/liverRegression/regression_liver/data/chrom_sizes/' + species + '_chrom_sizes.json', 'w') as f:\n",
    "    json.dump(chrom_map, f)\n",
    "# narrowPeak_dir = '/home/azstephe/liverRegression/regression_liver/data/raw/'\n",
    "\n",
    "# cols = ['chrom', 'start', 'end', 'name', 'score', 'strand', 'signal', 'p', 'q', 'peak_offset']\n",
    "# df = pd.read_csv(narrowPeak_dir + narrowPeak_file, sep='\\t', names=cols)\n",
    "\n",
    "# extension = 196608\n",
    "# df['summit'] = df['start'] + df['peak_offset']\n",
    "# df['new_start'] = df['summit'] - extension\n",
    "# df['new_end'] = df['summit'] + extension\n",
    "\n",
    "# # 4. Filter for boundaries\n",
    "# # Get the max length for the specific chromosome of each peak\n",
    "# df['max_len'] = df['chrom'].map(chrom_map)\n",
    "\n",
    "# # Keep only if start >= 0 AND end <= chromosome length\n",
    "# valid_sections = df[(df['new_start'] >= 0) & (df['new_end'] <= df['max_len'])].copy()\n",
    "\n",
    "# print(f\"Original peaks: {len(df)}\")\n",
    "# print(f\"Peaks kept: {len(valid_sections)}\")\n",
    "# print(f\"Peaks lost: {len(df) - len(valid_sections)}\")\n",
    "\n",
    "# outdir = '/home/azstephe/liverRegression/regression_liver/data/enformer_inputs/'\n",
    "# valid_sections[['chrom', 'new_start', 'new_end', 'name', 'signal']].to_csv(outdir + species + '_liver_pos_ALL_refSeqName.narrowPeak', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50c58d72-9846-4c27-9732-ad2e555b6dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3143 entries from original JSON.\n",
      "âœ… Created /home/azstephe/liverRegression/regression_liver/data/chrom_sizes/cow_chrom_sizes.json\n",
      "âœ… Created /home/azstephe/liverRegression/regression_liver/data/chrom_sizes/cow_chrom_names.sizes\n",
      "\n",
      "--- Preview of New Mapping ---\n",
      "Chromosome chr1: 158972876 bp\n",
      "Chromosome chr2: 137479425 bp\n",
      "Chromosome chr3: 121888057 bp\n",
      "Chromosome chr4: 121284772 bp\n",
      "Chromosome chr5: 121584146 bp\n"
     ]
    }
   ],
   "source": [
    "mapping_file_path = '/home/azstephe/liverRegression/regression_liver/data/genomes/ChromNameToRefSeqName_Btau_5.0.1.txt'\n",
    "input_json_path = '/home/azstephe/liverRegression/regression_liver/data/chrom_sizes/cow_chrom_sizes_RefSeq.json'\n",
    "output_json_path = '/home/azstephe/liverRegression/regression_liver/data/chrom_sizes/cow_chrom_sizes.json'\n",
    "output_txt_path = '/home/azstephe/liverRegression/regression_liver/data/chrom_sizes/cow_chrom_names.sizes'\n",
    "\n",
    "# 2. Load the mapping file {RefSeq: ChromName}\n",
    "# Assumption based on filename: Col 0 = ChromName, Col 1 = RefSeqName\n",
    "mapping = {}\n",
    "if os.path.exists(mapping_file_path):\n",
    "    with open(mapping_file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip() and not line.startswith('#'):\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    chrom_name = parts[0]\n",
    "                    refseq_id = parts[1]\n",
    "                    mapping[refseq_id] = chrom_name\n",
    "\n",
    "if os.path.exists(input_json_path):\n",
    "    with open(input_json_path, 'r') as f:\n",
    "        old_data = json.load(f)\n",
    "    print(f\"Loaded {len(old_data)} entries from original JSON.\")\n",
    "\n",
    "    # 4. Create the new dictionary with ChromNames\n",
    "    # We only keep keys that exist in our mapping file\n",
    "    new_data = {mapping[k]: v for k, v in old_data.items() if k in mapping}\n",
    "    \n",
    "    # 5. Save the results\n",
    "    # Save as JSON\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(new_data, f, indent=4)\n",
    "        \n",
    "    # Save as standard tab-separated .sizes file\n",
    "    with open(output_txt_path, 'w') as f:\n",
    "        for k, v in sorted(new_data.items()):\n",
    "            f.write(f\"{k}\\t{v}\\n\")\n",
    "            \n",
    "    print(f\"âœ… Created {output_json_path}\")\n",
    "    print(f\"âœ… Created {output_txt_path}\")\n",
    "    \n",
    "    # 6. Preview the first 5 entries\n",
    "    print(\"\\n--- Preview of New Mapping ---\")\n",
    "    for i, (k, v) in enumerate(list(new_data.items())[:5]):\n",
    "        print(f\"Chromosome {k}: {v} bp\")\n",
    "else:\n",
    "    print(f\"âŒ Error: Input JSON {input_json_path} not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "306ac566-e11f-493c-8c90-41816fb65d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_pos_LiuAll/cow_liver_TEST_500bp.bed\n",
      "Original peaks: 1284\n",
      "Peaks kept: 1280\n",
      "Peaks lost: 4\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_pos_LiuAll/macaque_liver_TEST_500bp.bed\n",
      "Original peaks: 2624\n",
      "Peaks kept: 2619\n",
      "Peaks lost: 5\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_pos_LiuAll/rat_liver_TEST_500bp.bed\n",
      "Original peaks: 2576\n",
      "Peaks kept: 2575\n",
      "Peaks lost: 1\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_pos_LiuAll/pig_liver_TEST_500bp.bed\n",
      "Original peaks: 1627\n",
      "Peaks kept: 1624\n",
      "Peaks lost: 3\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/neg/pig_liver_TEST_500bp.bed\n",
      "Original peaks: 3057\n",
      "Peaks kept: 2860\n",
      "Peaks lost: 197\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/neg/rat_liver_TEST_500bp.bed\n",
      "Original peaks: 4405\n",
      "Peaks kept: 4404\n",
      "Peaks lost: 1\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/neg/macaque_liver_TEST_500bp.bed\n",
      "Original peaks: 3477\n",
      "Peaks kept: 3470\n",
      "Peaks lost: 7\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/neg/cow_liver_TEST_500bp.bed\n",
      "Original peaks: 3720\n",
      "Peaks kept: 2969\n",
      "Peaks lost: 751\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/neg/mouse_liver_TEST_500bp.bed\n",
      "Original peaks: 3698\n",
      "Peaks kept: 3698\n",
      "Peaks lost: 0\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_LiuAll_test1/macaque_liver_TEST_500bp.bed\n",
      "Original peaks: 1157\n",
      "Peaks kept: 1155\n",
      "Peaks lost: 2\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_LiuAll_test1/cow_liver_TEST_500bp.bed\n",
      "Original peaks: 1096\n",
      "Peaks kept: 1096\n",
      "Peaks lost: 0\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_LiuAll_test1/pig_liver_TEST_500bp.bed\n",
      "Original peaks: 940\n",
      "Peaks kept: 871\n",
      "Peaks lost: 69\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_LiuAll_test1/rat_liver_TEST_500bp.bed\n",
      "Original peaks: 1241\n",
      "Peaks kept: 1241\n",
      "Peaks lost: 0\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_test2/macaque_liver_TEST_500bp.bed\n",
      "Original peaks: 472\n",
      "Peaks kept: 472\n",
      "Peaks lost: 0\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_test2/pig_liver_TEST_500bp.bed\n",
      "Original peaks: 293\n",
      "Peaks kept: 293\n",
      "Peaks lost: 0\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_test2/rat_liver_TEST_500bp.bed\n",
      "Original peaks: 723\n",
      "Peaks kept: 723\n",
      "Peaks lost: 0\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_test2/cow_liver_TEST_500bp.bed\n",
      "Original peaks: 337\n",
      "Peaks kept: 336\n",
      "Peaks lost: 1\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_test3/rat_liver_TEST_500bp.bed\n",
      "Original peaks: 1025\n",
      "Peaks kept: 1024\n",
      "Peaks lost: 1\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_test3/pig_liver_TEST_500bp.bed\n",
      "Original peaks: 716\n",
      "Peaks kept: 715\n",
      "Peaks lost: 1\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_test3/cow_liver_TEST_500bp.bed\n",
      "Original peaks: 694\n",
      "Peaks kept: 694\n",
      "Peaks lost: 0\n",
      "/home/azstephe/liverRegression/regression_liver/data/test_splits/log_test3/macaque_liver_TEST_500bp.bed\n",
      "Original peaks: 1263\n",
      "Peaks kept: 1261\n",
      "Peaks lost: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "keyword = '_liver_TEST_500bp.bed'\n",
    "test_dir = '/home/azstephe/liverRegression/regression_liver/data/test_splits/'\n",
    "\n",
    "out_dir = '/home/azstephe/liverRegression/regression_liver/data/enformer_inputs/'\n",
    "\n",
    "for d in ['log_pos_LiuAll', 'neg', 'log_LiuAll_test1', 'log_test2', 'log_test3']:\n",
    "    dr = os.path.join(test_dir, d)\n",
    "    for file in os.listdir(dr):\n",
    "        filepath = os.path.join(dr, file)\n",
    "        if 'non' not in filepath and keyword in filepath:\n",
    "            species = filepath.split('/')[-1].split('_')[0]\n",
    "            cols = ['chrom', 'start', 'end', 'peak', 'signal']\n",
    "            df = pd.read_csv(filepath, sep='\\t', names=cols)\n",
    "            \n",
    "            extension = 196608\n",
    "            df['new_start'] = df['start'] - extension + 250\n",
    "            df['new_end'] = df['end'] + extension - 250\n",
    "\n",
    "            with open('/home/azstephe/liverRegression/regression_liver/data/chrom_sizes/' + species + '_chrom_sizes.json', 'r') as f:\n",
    "                chrom_map = json.load(f)\n",
    "\n",
    "            df['max_len'] = df['chrom'].map(chrom_map)\n",
    "\n",
    "            valid_sections = df[(df['new_start'] >= 0) & (df['new_end'] <= df['max_len'])].copy()\n",
    "\n",
    "            print(filepath)\n",
    "            print(f\"Original peaks: {len(df)}\")\n",
    "            print(f\"Peaks kept: {len(valid_sections)}\")\n",
    "            print(f\"Peaks lost: {len(df) - len(valid_sections)}\")\n",
    "\n",
    "            out_dr = os.path.join(out_dir, d)\n",
    "            if not os.path.exists(out_dr):\n",
    "                os.makedirs(out_dr)\n",
    "            valid_sections[['chrom', 'new_start', 'new_end', 'peak', 'signal']].to_csv(os.path.join(out_dr, file), sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6f073f6-96e8-40d5-b5b8-af0d57146717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‚ Processing directory: log_pos_LiuAll\n",
      "Extracting: cow_liver_TEST_500bp.bed -> cow_liver_TEST_500bp.fa\n",
      "Extracting: macaque_liver_TEST_500bp.bed -> macaque_liver_TEST_500bp.fa\n",
      "Extracting: pig_liver_TEST_500bp.bed -> pig_liver_TEST_500bp.fa\n",
      "Extracting: rat_liver_TEST_500bp.bed -> rat_liver_TEST_500bp.fa\n",
      "\n",
      "ðŸ“‚ Processing directory: neg\n",
      "Extracting: pig_liver_TEST_500bp.bed -> pig_liver_TEST_500bp.fa\n",
      "Extracting: rat_liver_TEST_500bp.bed -> rat_liver_TEST_500bp.fa\n",
      "Extracting: mouse_liver_TEST_500bp.bed -> mouse_liver_TEST_500bp.fa\n",
      "Extracting: cow_liver_TEST_500bp.bed -> cow_liver_TEST_500bp.fa\n",
      "Extracting: macaque_liver_TEST_500bp.bed -> macaque_liver_TEST_500bp.fa\n",
      "\n",
      "ðŸ“‚ Processing directory: log_LiuAll_test1\n",
      "Extracting: macaque_liver_TEST_500bp.bed -> macaque_liver_TEST_500bp.fa\n",
      "Extracting: cow_liver_TEST_500bp.bed -> cow_liver_TEST_500bp.fa\n",
      "Extracting: pig_liver_TEST_500bp.bed -> pig_liver_TEST_500bp.fa\n",
      "Extracting: rat_liver_TEST_500bp.bed -> rat_liver_TEST_500bp.fa\n",
      "\n",
      "ðŸ“‚ Processing directory: log_test2\n",
      "Extracting: macaque_liver_TEST_500bp.bed -> macaque_liver_TEST_500bp.fa\n",
      "Extracting: cow_liver_TEST_500bp.bed -> cow_liver_TEST_500bp.fa\n",
      "Extracting: pig_liver_TEST_500bp.bed -> pig_liver_TEST_500bp.fa\n",
      "Extracting: rat_liver_TEST_500bp.bed -> rat_liver_TEST_500bp.fa\n",
      "\n",
      "ðŸ“‚ Processing directory: log_test3\n",
      "Extracting: cow_liver_TEST_500bp.bed -> cow_liver_TEST_500bp.fa\n",
      "Extracting: macaque_liver_TEST_500bp.bed -> macaque_liver_TEST_500bp.fa\n",
      "Extracting: rat_liver_TEST_500bp.bed -> rat_liver_TEST_500bp.fa\n",
      "Extracting: pig_liver_TEST_500bp.bed -> pig_liver_TEST_500bp.fa\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "# --- Configuration ---\n",
    "# The parent directory containing your subfolders\n",
    "base_dir = '/home/azstephe/liverRegression/regression_liver/data/enformer_inputs/'\n",
    "sub_dirs = ['log_pos_LiuAll', 'neg', 'log_LiuAll_test1', 'log_test2', 'log_test3']\n",
    "# sub_dirs = []\n",
    "\n",
    "genomes = {\n",
    "    'cow': '/home/azstephe/regression_liver/data/splits/cowMouse/GCF_000003205.7_Btau_5.0.1_genomic_chromName.fna',\n",
    "    'rat': '/projects/pfenninggroup/machineLearningForComputationalBiology/regElEvoGrant/RatGenome/rn6.fa',\n",
    "    'macaque': '/projects/pfenninggroup/machineLearningForComputationalBiology/regElEvoGrant/MacaqueGenome/rheMac8.fa',\n",
    "    'mouse': '/projects/pfenninggroup/machineLearningForComputationalBiology/regElEvoGrant/MouseGenome/mm10.fa',\n",
    "    'pig': '/data/pfenninggroup/machineLearningForComputationalBiology/regElEvoGrant/200MammalsFastas/susScr3.fa'\n",
    "}\n",
    "\n",
    "for d in sub_dirs:\n",
    "    dr = os.path.join(base_dir, d)\n",
    "    \n",
    "    if not os.path.exists(dr):\n",
    "        print(f\"âš ï¸ Directory not found, skipping: {dr}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\nðŸ“‚ Processing directory: {d}\")\n",
    "    \n",
    "    for file in os.listdir(dr):\n",
    "        # Only process BED or narrowPeak files\n",
    "        if file.endswith('.bed') or file.endswith('.narrowPeak'):\n",
    "            input_path = os.path.join(dr, file)\n",
    "            species = file.split('_')[0]\n",
    "            \n",
    "            # Create output filename by replacing extension with .fa\n",
    "            output_file = os.path.splitext(file)[0] + \".fa\"\n",
    "            output_path = os.path.join(dr, output_file)\n",
    "            \n",
    "            print(f\"Extracting: {file} -> {output_file}\")\n",
    "\n",
    "            genome_fasta = genomes[species]\n",
    "            # Construct the command\n",
    "            cmd = [\n",
    "                \"bedtools\", \"getfasta\",\n",
    "                \"-fi\", genome_fasta,\n",
    "                \"-bed\", input_path,\n",
    "                \"-fo\", output_path\n",
    "            ]\n",
    "            \n",
    "            #Run the command\n",
    "            try:\n",
    "                subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"  âŒ Error processing {file}: {e.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b147f506-aced-431e-aded-b0e304de5dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Batch Shape: (293, 393216, 4)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "# Assuming your one_hot_encode and prepare_input are defined\n",
    "# from enformer import one_hot_encode \n",
    "\n",
    "def prepare_input(sequence: str):\n",
    "    target_length = 393216\n",
    "    if len(sequence) != target_length:\n",
    "        print(\"input length != 393216\")\n",
    "        return\n",
    "\n",
    "    # Use your encoding function\n",
    "    encoded = one_hot_encode(sequence)\n",
    "    \n",
    "    # Add the Batch Dimension: (393216, 4) -> (1, 393216, 4)\n",
    "    return encoded[np.newaxis, ...]\n",
    "    \n",
    "def process_fasta_to_batch(fasta_path):\n",
    "    encoded_list = []\n",
    "    \n",
    "    # 1. Iterate through the FASTA file\n",
    "    for record in SeqIO.parse(fasta_path, \"fasta\"):\n",
    "        seq = str(record.seq).upper()\n",
    "        \n",
    "        # 2. Use your prepare_input logic\n",
    "        # Note: prepare_input already adds the (1, 393216, 4) dimension\n",
    "        encoded_seq = prepare_input(seq)\n",
    "        \n",
    "        if encoded_seq is not None:\n",
    "            encoded_list.append(encoded_seq)\n",
    "    \n",
    "    # 3. Stack them into a single batch\n",
    "    # Since prepare_input returns (1, L, 4), we use concatenate on axis 0\n",
    "    if encoded_list:\n",
    "        batch_array = np.concatenate(encoded_list, axis=0)\n",
    "        return batch_array\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# Usage\n",
    "fasta_file = \"/home/azstephe/liverRegression/regression_liver/data/enformer_inputs/log_test2/pig_liver_TEST_500bp.fa\"\n",
    "x = process_fasta_to_batch(fasta_file)\n",
    "\n",
    "print(f\"Final Batch Shape: {x.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7179130-793b-467c-be60-21cc305f61d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 393216, 4)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c2e35f1-8825-4c03-9927-3a7ec10ef3ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'serving_default'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/azstephe/liverRegression/repos/enformer/enformer_model_local\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 2. Define the Enformer Wrapper Class (Standard for this model)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# This handles the complex \"signatures\" that Enformer uses\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# class Enformer:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 3. Initialize the model using your LOCAL path\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mEnformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m, in \u001b[0;36mEnformer.__init__\u001b[0;34m(self, model_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_path):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Load the local directory directly\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mserving_default\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer_env/lib/python3.9/site-packages/tensorflow/python/saved_model/signature_serialization.py:247\u001b[0m, in \u001b[0;36m_SignatureMap.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 247\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_signatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'serving_default'"
     ]
    }
   ],
   "source": [
    "model_path = '/home/azstephe/liverRegression/repos/enformer/enformer_model_local'\n",
    "\n",
    "# 2. Define the Enformer Wrapper Class (Standard for this model)\n",
    "# This handles the complex \"signatures\" that Enformer uses\n",
    "# class Enformer:\n",
    "#     def __init__(self, model_path):\n",
    "#         # Load the local directory directly\n",
    "#         self._model = hub.load(model_path).signatures['serving_default']\n",
    "\n",
    "#     def predict_on_batch(self, inputs):\n",
    "#         # Ensure inputs are float32 to save memory\n",
    "#         inputs = tf.cast(inputs, tf.float32)\n",
    "#         return self._model(inputs)\n",
    "\n",
    "# 3. Initialize the model using your LOCAL path\n",
    "model = Enformer(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccdbfe35-c9c4-4cd0-b79a-3adc2d3b11cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "/tmp/tfhub_modules/c444fdff3e183daf686869692c26e00391f6773c/tfhub_module.pb; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m enformer \u001b[38;5;241m=\u001b[39m \u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://tfhub.dev/deepmind/enformer/1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m inputs \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m enformer\u001b[38;5;241m.\u001b[39mpredict_on_batch(inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer_env/lib/python3.9/site-packages/tensorflow_hub/module.py:157\u001b[0m, in \u001b[0;36mModule.__init__\u001b[0;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a Module to be used in the current graph.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mThis creates the module `state-graph` under an unused variable_scope based\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m  tf.errors.NotFoundError: if the requested graph contains unknown ops.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spec \u001b[38;5;241m=\u001b[39m \u001b[43mas_module_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainable \u001b[38;5;241m=\u001b[39m trainable\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(tags \u001b[38;5;129;01mor\u001b[39;00m [])\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer_env/lib/python3.9/site-packages/tensorflow_hub/module.py:30\u001b[0m, in \u001b[0;36mas_module_spec\u001b[0;34m(spec)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m spec\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(spec, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 30\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_module_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown module spec type: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(spec))\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer_env/lib/python3.9/site-packages/tensorflow_hub/module.py:65\u001b[0m, in \u001b[0;36mload_module_spec\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loads a ModuleSpec from a TF Hub service or the filesystem.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03mWarning: Deprecated. This belongs to the hub.Module API and TF1 Hub format.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m  tf.errors.OpError: on file handling exceptions.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m path \u001b[38;5;241m=\u001b[39m registry\u001b[38;5;241m.\u001b[39mresolver(path)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer_env/lib/python3.9/site-packages/tensorflow_hub/registry.py:51\u001b[0m, in \u001b[0;36mMultiImplRegister.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m impl \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impls):\n\u001b[1;32m     50\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m impl\u001b[38;5;241m.\u001b[39mis_supported(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     fails\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtype\u001b[39m(impl)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer_env/lib/python3.9/site-packages/tensorflow_hub/native_module.py:139\u001b[0m, in \u001b[0;36mLoader.__call__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[0;32m--> 139\u001b[0m   module_def_proto \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module_def_proto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m module_def_proto\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m!=\u001b[39m module_def_pb2\u001b[38;5;241m.\u001b[39mModuleDef\u001b[38;5;241m.\u001b[39mFORMAT_V3:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported module def format: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    143\u001b[0m                      module_def_proto\u001b[38;5;241m.\u001b[39mformat)\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer_env/lib/python3.9/site-packages/tensorflow_hub/native_module.py:130\u001b[0m, in \u001b[0;36mLoader._get_module_def_proto\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    128\u001b[0m module_def_proto \u001b[38;5;241m=\u001b[39m module_def_pb2\u001b[38;5;241m.\u001b[39mModuleDef()\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mOpen(module_def_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 130\u001b[0m   module_def_proto\u001b[38;5;241m.\u001b[39mParseFromString(\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module_def_proto\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer_env/lib/python3.9/site-packages/tensorflow/python/lib/io/file_io.py:117\u001b[0m, in \u001b[0;36mFileIO.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    106\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the contents of a file as a string.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m  Starts reading from current position in file.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;03m    string if in string (regular) mode.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preread_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    119\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer_env/lib/python3.9/site-packages/tensorflow/python/lib/io/file_io.py:79\u001b[0m, in \u001b[0;36mFileIO._preread_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_check_passed:\n\u001b[1;32m     77\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mPermissionDeniedError(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     78\u001b[0m                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt open for reading\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_buf \u001b[38;5;241m=\u001b[39m \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBufferedInputStream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: /tmp/tfhub_modules/c444fdff3e183daf686869692c26e00391f6773c/tfhub_module.pb; No such file or directory"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "enformer = hub.Module('https://tfhub.dev/deepmind/enformer/1')\n",
    "\n",
    "inputs = x[0:1]\n",
    "predictions = enformer.predict_on_batch(inputs)\n",
    "model = Enformer(model_path)\n",
    "predictions = model.predict_on_batch(sequence_one_hot[np.newaxis])['human'][0]\n",
    "# predictions['human'].shape  # [batch_size, 896, 5313]\n",
    "# predictions[mouse].shape \n",
    "\n",
    "# # The outputs are a dictionary: 'human' and 'mouse'\n",
    "# human_predictions = outputs['human'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd4563b5-e22e-4280-86c6-ce624dc08b3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'serving_default'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m hub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://tfhub.dev/deepmind/enformer/1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 2. Get the prediction function (signature)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m enformer_predict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignatures\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mserving_default\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# # 3. Prepare your input tensor\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# # Ensure x is a float32 numpy array or tensor of shape (batch, 393216, 4)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# input_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# # 'human' shape: [batch, 896, 5313]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# human_predictions = outputs['human'].numpy()\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/enformer_env/lib/python3.9/site-packages/tensorflow/python/saved_model/signature_serialization.py:247\u001b[0m, in \u001b[0;36m_SignatureMap.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 247\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_signatures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'serving_default'"
     ]
    }
   ],
   "source": [
    "model = hub.load(\"https://tfhub.dev/deepmind/enformer/1\")\n",
    "\n",
    "# 2. Get the prediction function (signature)\n",
    "enformer_predict = model.signatures['serving_default']\n",
    "\n",
    "# # 3. Prepare your input tensor\n",
    "# # Ensure x is a float32 numpy array or tensor of shape (batch, 393216, 4)\n",
    "# input_tensor = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "\n",
    "# # 4. Run the prediction\n",
    "# outputs = enformer_predict(input_tensor)\n",
    "\n",
    "# # 5. Extract the predictions\n",
    "# # 'human' shape: [batch, 896, 5313]\n",
    "# human_predictions = outputs['human'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef37537a-2732-4751-b5a9-b9b13b3d18f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available signatures: []\n"
     ]
    }
   ],
   "source": [
    "print(\"Available signatures:\", list(enformer_model.signatures.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b25574b-ea3e-4327-a918-5fdc6205d0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enformer",
   "language": "python",
   "name": "enformer_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
